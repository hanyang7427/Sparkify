{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "Sparkify是一家数字音乐服务的公司，大量用户每天使用该服务听自己喜欢的歌曲，包括游客，免费用户，付费用户。这个notebook探索了和用户流失有相关性的特征，并构建了通过用户的行为数据预测用户是否会流失的模型，最后列出了会影响用户是否会流失的重要特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "用户每次和Sparkify服务互动都会产生用户行为数据，例如听歌曲，访问页面，添加好友，点赞等。完整的数据是12G，此notebook使用数据的一个子集(128M)，在非集群环境运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import udf, expr, col, isnan, max, avg, min, count, countDistinct, sqrt, lit, when\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import sum as fsum\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "base_color = sb.color_palette()[0]\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Sparkify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载和清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/hanyang/Downloads/spark_proj/Sparkify/mini_sparkify_event_data.json;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.json.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/hanyang/Downloads/spark_proj/Sparkify/mini_sparkify_event_data.json;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:392)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6be68238c2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mini_sparkify_event_data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'数据集有{}行，{}列。'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/hanyang/Downloads/spark_proj/Sparkify/mini_sparkify_event_data.json;'"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"mini_sparkify_event_data.json\")\n",
    "df.printSchema()\n",
    "print('数据集有{}行，{}列。'.format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个按百分比显示null的函数\n",
    "def show_null_percentage(df):\n",
    "    '''\n",
    "    INPUT\n",
    "    df - spark DataFrame\n",
    "    dicimal - How many decimal to keep\n",
    "    \n",
    "    OUTPUT\n",
    "    None - print the non_null percentage of each column\n",
    "    '''\n",
    "    count = df.count()\n",
    "    statistics = df.describe().take(1)\n",
    "    statistics_df = spark.createDataFrame(statistics)\n",
    "    columns = statistics_df.columns\n",
    "    kv = zip(columns[1:], (('/{},'.format(count).join(columns[1:]))+'/{}'.format(count)).split(','))\n",
    "    statistics_df.select([(1-expr(v)).alias(k) for k,v in kv]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 使用定义的helper function查看缺失值比率\n",
    "show_null_percentage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上边观察到，artist / length / song的缺失比率一样，firstName / gender / lastName / location / registration / userAgent缺失比率一样，通过给这些行添加索引号，对比索引号发现它们缺失的是相同的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个比较两个列是否一致的函数\n",
    "def column_consistent(df, kvs, how='inner'):\n",
    "    '''\n",
    "    INPUT\n",
    "    df - spark DataFrame\n",
    "    kvs - pair of column and value, compare two column's consistency. type:dict\n",
    "          available dict key: columns in df.columns\n",
    "          available dict value: 'null' / 'notnull' / 'nan' / 'value|value' / string / !string / number\n",
    "          eg. {'animal':'human','like':'money'}: all human like money，all like mone are human\n",
    "    how - inner / outer / left / right\n",
    "    \n",
    "    OUTPUT\n",
    "    return the consistency of two columns.\n",
    "    \n",
    "    '''\n",
    "    ks = list(kvs.keys())\n",
    "    vs = list(kvs.values())\n",
    "    df_index = df.select(\"*\").withColumn(\"index\", monotonically_increasing_id())\n",
    "    switch = {\n",
    "        \"null\": lambda *args: df_index[args[0]].isNull(),\n",
    "        \"nan\": lambda *args: isnan(df_index[args[0]]),\n",
    "        'notnull': lambda *args: df_index[args[0]].isNotNull(),\n",
    "        'isin': lambda *args: df_index[args[0]].isin(args[1].split('|')),\n",
    "    }\n",
    "    id_pair = {\n",
    "                k : df_index\\\n",
    "               .select(col('index').alias('{}={}_index'.format(k,v)))\\\n",
    "               .where((switch.get('isin' if '|' in v else v, lambda *args:df_index[args[0]]!=args[1].strip('!') if '!' in args[1] else df_index[args[0]]==args[1]))(k,v)) \\\n",
    "               for k,v in kvs.items()\n",
    "              }\n",
    "    result = id_pair[ks[0]].join(id_pair[ks[1]], id_pair[ks[0]]['{}={}_index'.format(ks[0],vs[0])]==id_pair[ks[1]]['{}={}_index'.format(ks[1],vs[1])], 'outer')\n",
    "    if result.where(result[result.columns[0]].isNull() | result[result.columns[1]].isNull()).count() == 0:\n",
    "        print('if \"{}={}\", then \"{}={}\". if \"{}={}\", then \"{}={}\".'.format(ks[0], vs[0], ks[1], vs[1], ks[1], vs[1], ks[0],vs[0]))\n",
    "    if how == 'inner':\n",
    "        return result.where(result[result.columns[0]].isNotNull() & result[result.columns[1]].isNotNull())\n",
    "    if how == 'outer':\n",
    "        return result\n",
    "    if how == 'left':\n",
    "        return result.where(result[result.columns[0]].isNotNull())\n",
    "    if how == 'right':\n",
    "        return result.where(result[result.columns[1]].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# artist缺失，length缺失，song缺失的行是相同\n",
    "column_consistent(df, {'artist':'null', 'length':'null'})\n",
    "column_consistent(df, {'length':'null', 'song':'null'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# firstname缺失，gender缺失，lastname缺失，location缺失，registration缺失，userAgent缺失的行是相同的\n",
    "column_consistent(df, {'firstname':'null', 'gender':'null'})\n",
    "column_consistent(df, {'gender':'null', 'lastname':'null'})\n",
    "column_consistent(df, {'lastname':'null', 'location':'null'})\n",
    "column_consistent(df, {'location':'null', 'registration':'null'})\n",
    "column_consistent(df, {'registration':'null', 'userAgent':'null'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page=Cancellation Confirmation的行 和 auth=Cancelled的行是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_consistent(df, {'page':'Cancellation Confirmation', 'auth':'Cancelled'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详细看一下artist和length和song有缺失值的行，发现这些用户处在没有听任何歌曲的状态(song=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where((df['artist'].isNull())).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详细看下firstName和gender和lastName和location和registration和userAgen有缺失值的行，发现这些是没有注册的游客(registration=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where((df['firstName'].isNull())).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userId存在空字符串的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.select('userId').dropDuplicates().sort('userId').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_consistent(df, {'userId':'', 'registration':'null'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userId是空字符串的数量与游客数量相等，且是相同的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('数据中userId是空字符串的数量是： {}'.format(df.where(df.userId == '').count()))\n",
    "print('游客的数量是：', df.where(df.registration.isNull()).count())\n",
    "column_consistent(df, {'userId':'','registration':'null'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "业务理解1：分别抽取几个访问过Cancellation Confirmation页面的付费和免费用户(游客不会访问Cancellation Confirmation页面)，观察他们访问Cancellation Confirmation页面之后的行为，发现在访问Cancellation Confirmation之后，就没有任何活动了，可以理解为用户注销了，或者用户churn。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出访问过Cancellation Confirmation的paid和free用户\n",
    "df.select('userId','level').where(df.level=='paid').where(df.page=='Cancellation Confirmation').show(5)\n",
    "df.select('userId','level').where(df.level=='free').where(df.page=='Cancellation Confirmation').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看paid用户userId=18最后的活动\n",
    "df.select('level','page').where(df.userId=='18').collect()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看paid用户userId=32最后的活动\n",
    "df.select('level','page').where(df.userId=='32').collect()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看free用户userId=125最后的活动\n",
    "df.select('level','page').where(df.userId=='125').collect()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看free用户userId=143最后的活动\n",
    "df.select('level','page').where(df.userId=='143').collect()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "业务理解2：游客的level也可以是paid，可能是游客购买了单曲，被标识为了paid用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看level是paid，用户名是空字符串的记录\n",
    "df.select('userId','level','registration').where(df.userId=='').where(df.level=='paid').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于游客永远不会访问Cancellation Confirmation，对于预测churn来说没有意义，将游客的数据删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除registration是空的行，即删除游客的记录\n",
    "df_valid = df.dropna(subset='registration')\n",
    "df_valid.select('userId').dropDuplicates().sort('userId').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('清洗后数据集有{}行，{}列'.format(df_valid.count(), len(df_valid.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加新列churn_page，如果page=Cancellation Confirmation，churn_page=1\n",
    "transform_churn = udf(lambda x:1 if x=='Cancellation Confirmation' else 0, IntegerType())\n",
    "df_valid = df_valid.withColumn('churn_page', transform_churn('page'))\n",
    "# 按用户分组，根据churn_page得到用户是否流失(churn列)\n",
    "windowval = Window.partitionBy('userId')\n",
    "df_valid = df_valid.withColumn('churn', max('churn_page').over(windowval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df_valid.select('userId',col('churn').alias('label'))\\\n",
    "    .dropDuplicates(subset=['userId']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.countplot(x='label',data=label_df);\n",
    "plt.title('most user do not churn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到没有churn的客户远多于churn的客户，label是偏态分布的，这可能会影响预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索和churn相关的特征\n",
    "通过从数据中提取用户的特征，观察用户特征和churn的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "churn_per_level_df = df_valid.where(df_valid['page']=='Cancellation Confirmation').groupBy('level').count().toPandas()\n",
    "plt.bar(churn_per_level_df['level'], churn_per_level_df['count']);\n",
    "plt.ylabel('churn count');\n",
    "plt.title('Paid users are more likely to churn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 计算男女的churn比率，保存在churn_ratio列\n",
    "w = Window.partitionBy('gender')\n",
    "gender_churn_ratio_df = df_valid.groupBy('churn','gender').count()\\\n",
    "    .withColumn('total',fsum('count').over(w))\\\n",
    "    .select('gender',(col('count')/col('total')).alias('churn_ratio'))\\\n",
    "    .where(df_valid.churn==1)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bar = plt.bar(gender_churn_ratio_df['gender'],gender_churn_ratio_df['churn_ratio'])\n",
    "for p in bar.patches:\n",
    "    plt.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                 ha = 'center', va = 'top', xytext = (0, 10), textcoords = 'offset points')\n",
    "plt.ylabel('churn_ratio');\n",
    "plt.title('Male users are more likely to churn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 用户注册时长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_since_reg_df = df_valid.select(((col('ts') - col('registration'))/1000/60/60).alias('hours_since_reg'),'churn','userId')\\\n",
    "    .groupBy('churn','userId').agg(max('hours_since_reg'))\\\n",
    "    .withColumnRenamed('max(hours_since_reg)','hours_since_reg')\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.boxplot('hours_since_reg', 'churn', data=hours_since_reg_df, orient=\"h\");\n",
    "plt.title('users with short hours_since_reg are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.distplot(hours_since_reg_df['hours_since_reg']);\n",
    "plt.title('hours_since_reg is Normal distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 用户会话会中song数量的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_song_count_df = df_valid.where(df_valid['song'] != '')\\\n",
    "    .groupBy('userId','sessionId','churn')\\\n",
    "    .agg(count('song').alias('song_count'))\\\n",
    "    .groupBy('userId','churn')\\\n",
    "    .agg(avg('song_count').alias('avg_song_count'))\\\n",
    "    .select('churn','avg_song_count').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.boxplot('avg_song_count','churn',data=avg_song_count_df, orient=\"h\");\n",
    "plt.title('users with less avg_song_count are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.distplot(avg_song_count_df['avg_song_count']);\n",
    "plt.title('avg_song_count is Normal distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 用户的会话数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_per_user_df = df_valid.groupBy('userId','churn')\\\n",
    "    .agg(countDistinct('sessionId').alias('unique_session'))\\\n",
    "    .withColumn('unique_session_sqrt',sqrt('unique_session'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.boxplot('unique_session','churn',data=session_per_user_df, orient=\"h\");\n",
    "plt.title('users with less unique_session are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_per_user_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.distplot(session_per_user_df['unique_session']);\n",
    "plt.title('unique_session is Skewed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会话数量是偏态分布的，这里进行开平方处理，处理后近似正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(session_per_user_df['unique_session_sqrt']);\n",
    "plt.title('after sqrt unique_session is Normal distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 用户将歌曲添加到播放列表的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "play_list_df = df_valid.where(df_valid.page=='Add to Playlist')\\\n",
    "    .groupBy('userId','churn').agg(count('userId').alias('num_playlist'))\\\n",
    "    .withColumn('num_playlist_sqrt',sqrt('num_playlist'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.boxplot('num_playlist','churn',data=play_list_df, orient=\"h\");\n",
    "plt.title('users with less num_playlist are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.distplot(play_list_df['num_playlist']);\n",
    "plt.title('num_playlist is Skewed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加到播放列表的数量是偏态分布的，这里进行开平方处理，处理后近似正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.distplot(play_list_df['num_playlist_sqrt']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 用户添加好友数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_friend_df = df_valid.where(df_valid.page=='Add Friend')\\\n",
    "    .groupBy('userId','churn').agg(count('userId').alias('num_friend'))\\\n",
    "     .withColumn('num_friend_sqrt',sqrt('num_friend'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.boxplot('num_friend','churn',data=add_friend_df, orient=\"h\");\n",
    "plt.title('users with less num_friend are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.distplot(add_friend_df['num_friend']);\n",
    "plt.title('num_friend is Skewed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好友数量是偏态分布的，这里进行开平方处理，处理后近似正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.distplot(np.sqrt(add_friend_df['num_friend_sqrt']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 没有听歌曲的比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = df_valid.groupBy('userId','churn').agg(count('length').alias('row_count'))\n",
    "length_count = df_valid.groupBy('userId').count()\n",
    "null_ratio_df = row_count.join(length_count,row_count.userId==length_count.userId,'inner')\\\n",
    "    .withColumn('null_ratio',(1-(col('row_count')/col('count'))))\\\n",
    "    .select('churn','null_ratio').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.boxplot('null_ratio','churn', data=null_ratio_df, orient=\"h\");\n",
    "plt.title('users with more num_friend are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.distplot(null_ratio_df['null_ratio']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### churn 和 最大itemInSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "itemInSession 与 sessionId有密切关系，itemInSession表示每个sessionId中有多少个item，通过观察某个sessionId的数据，发现itemInSession是递增的，这应该是统计的日志写入那一时刻session中的item数量，单看此列没有意义，这里计算会话中item的实际数量的最大值，观察该最大值与churn的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_itemInSession_df = df_valid\\\n",
    "    .groupBy('churn','userId','sessionId')\\\n",
    "    .agg(max('itemInSession').alias('totalItemInSession'))\\\n",
    "    .groupBy('churn','userId')\\\n",
    "    .agg(max('totalItemInSession').alias('maxItemInSession'))\\\n",
    "    .withColumn('maxItemInSession_sqrt',sqrt('maxItemInSession'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.boxplot('maxItemInSession','churn',data=max_itemInSession_df, orient=\"h\");\n",
    "plt.title('users with less maxItemInSession are more likely to churn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sb.distplot(max_itemInSession_df['maxItemInSession']);\n",
    "plt.title('maxItemInSession is Skewed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该特征是偏态分布的，这里进行开平方处理，处理后近似正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(max_itemInSession_df['maxItemInSession_sqrt']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程\n",
    "选取了探索过的9个特征作为算法的输入，将特征和标签保存在df_features中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户的level\n",
    "f1 = df_valid.select('userId','level').dropDuplicates(subset=['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 用户的性别\n",
    "f2 = df_valid.select('userId','gender').dropDuplicates(subset=['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户注册时长\n",
    "f3 = df_valid.select(((col('ts') - col('registration'))/1000/60/60).alias('hours_since_reg'),'userId')\\\n",
    "    .groupBy('userId').agg(max('hours_since_reg'))\\\n",
    "    .withColumnRenamed('max(hours_since_reg)','hours_since_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 用户所有会话听的歌曲数量均值\n",
    "f4 = avg_song_count_df = df_valid.where(df_valid['song'] != '')\\\n",
    "    .groupBy('userId','sessionId')\\\n",
    "    .agg(count('song').alias('song_count'))\\\n",
    "    .groupBy('userId')\\\n",
    "    .agg(avg('song_count').alias('avg_song_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户的会话数量\n",
    "f5 = df_valid.groupBy('userId')\\\n",
    "    .agg(countDistinct('sessionId').alias('unique_session_num'))\\\n",
    "    .withColumn('unique_session_num_sqrt',sqrt('unique_session_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户将歌曲添加到播放列表的数量\n",
    "f6 = df_valid.where(df_valid.page=='Add to Playlist')\\\n",
    "    .groupBy('userId').agg(count('page').alias('add_playlist_num'))\\\n",
    "    .withColumn('num_playlist_sqrt',sqrt('add_playlist_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 用户添加好友数量\n",
    "f7 = df_valid.where(df_valid.page=='Add Friend')\\\n",
    "    .groupBy('userId').agg(count('page').alias('add_friend_num'))\\\n",
    "    .withColumn('add_friend_num_sqrt',sqrt('add_friend_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没有听歌曲的比率\n",
    "row_count = df_valid.groupBy('userId').agg(count('length').alias('row_count'))\n",
    "length_count = df_valid.groupBy('userId').count()\n",
    "f8 = row_count.join(length_count, 'userId','inner')\\\n",
    "    .withColumn('null_ratio',(1-(col('row_count')/col('count'))))\\\n",
    "    .select('userId','null_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大itemInSession\n",
    "f9 = df_valid\\\n",
    "    .groupBy('userId','sessionId')\\\n",
    "    .agg(max('itemInSession').alias('totalItemInSession'))\\\n",
    "    .groupBy('userId').agg(max('totalItemInSession').alias('maxItemInSession'))\\\n",
    "    .withColumn('maxItemInSession_sqrt',sqrt('maxItemInSession'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df_valid.select('userId',col('churn').alias('label')).dropDuplicates(subset=['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_features = f1.join(f2,'userId','outer')\\\n",
    "    .join(f3,'userId','outer')\\\n",
    "    .join(f4,'userId','outer')\\\n",
    "    .join(f5,'userId','outer')\\\n",
    "    .join(f6,'userId','outer')\\\n",
    "    .join(f7,'userId','outer')\\\n",
    "    .join(f8,'userId','outer')\\\n",
    "    .join(f9,'userId','outer')\\\n",
    "    .join(label,'userId','outer')\\\n",
    "    .fillna(0).drop('userId').persist()\n",
    "df_features.show(n=1,vertical=True)\n",
    "print('样本数量：{}'.format(df_features.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模\n",
    "此部分将数据集分成训练集、测试集和验证集，使用F1 score 评估了四个算法：逻辑回归，GBT，支持向量机，随机森林。使用k折交叉验证选出表现最好的算法，然后使用网格搜索调整超参数训练出在训练集上表现最好的模型，最后用该模型在测试集上进行验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估指标\n",
    "使用F1 score作为评估指标，我们不希望出现预测是churn而实际不是的情况(这样可能会浪费资源)，也不希望预测不是churn而实际是churn的情况(这样可能会丢失用户)，所以使用F1 socre。由于标签中的churn数量远大于非churn的数量，模型的预测结果会出很多Fasle Negtaive(预测为0实际为1)的错误，使用F1 score可以让评估指标捕捉到这些错误。同时也使用Accuracy作为参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集\n",
    "60%作为训练集，20%作为验证集，20%作为测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, rest = df_features.randomSplit([0.6, 0.4], seed=42)\n",
    "test, validation = rest.randomSplit([0.5, 0.5], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基准模型\n",
    "将训练集中的数据全部预测为0，或者全部预测为1，作为基准模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = test.withColumn('prediction', lit(1.0))\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_1, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F1 Score:{}'.format(evaluator.evaluate(results_1, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_0 = test.withColumn('prediction', lit(0.0))\n",
    "print('Accuracy: {}'.format(evaluator.evaluate(results_0, {evaluator.metricName: \"accuracy\"})))\n",
    "print('F1 Score:{}'.format(evaluator.evaluate(results_0, {evaluator.metricName: \"f1\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测全部都是0的基准模型在测试集上也有较好表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型选择\n",
    "使用了pipeline (pipeline包含特征预处理)。使用交叉验证评估了四个模型，选择在验证集上表现最好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义使用的feature列表\n",
    "features = ['avg_song_count','unique_session_num','add_playlist_num','add_friend_num','maxItemInSession',\n",
    "            'null_ratio','hours_since_reg','gender_index','level_index']\n",
    "features_sqrt = ['avg_song_count','unique_session_num_sqrt','num_playlist_sqrt','add_friend_num_sqrt',\n",
    "            'maxItemInSession_sqrt','null_ratio','hours_since_reg','gender_index','level_index']\n",
    "\n",
    "# 使用StringIndexer处理分类变量\n",
    "indexer_gender = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
    "indexer_level = StringIndexer(inputCol=\"level\", outputCol=\"level_index\")\n",
    "# 创建特征向量\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"NumFeatures\")\n",
    "assembler_sqrt = VectorAssembler(inputCols=features_sqrt, outputCol=\"NumFeatures_sqrt\")\n",
    "# 使用StandardScaler缩放特征\n",
    "scaler_ss = StandardScaler(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures_ss\", withStd=True, withMean=False)\n",
    "scaler_ss_sqrt = StandardScaler(inputCol=\"NumFeatures_sqrt\", outputCol=\"ScaledNumFeatures_ss_sqrt\", withStd=True, withMean=False)\n",
    "# 定义评估器\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "# 定义网格搜索参数\n",
    "paramGrid = ParamGridBuilder().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化逻辑回归\n",
    "lr = LogisticRegression(maxIter=10,featuresCol='ScaledNumFeatures_ss', labelCol='label')\n",
    "# 定义逻辑回归的pipeline\n",
    "pipeline_lr = Pipeline(stages=[indexer_gender, indexer_level, assembler, scaler_ss, lr])\n",
    "s = time()\n",
    "Model_lr = pipeline_lr.fit(train)\n",
    "# 使用逻辑回归pipeline拟合训练数据\n",
    "e = time()\n",
    "print('time used:',e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_result = Model_lr.transform(validation)\n",
    "print('f1 score:',evaluator.evaluate(lr_result, {evaluator.metricName: \"f1\"}))\n",
    "print('accuracy:',evaluator.evaluate(lr_result, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于特征中有偏态数据，偏态特征违反了逻辑回归的假设，这里逻辑回归使用了开平方后的特征又拟合了一遍，但是accuracy和f1 score没有提升。后面算法均不使用原始特征(不使用开方后的特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化逻辑回归\n",
    "lr = LogisticRegression(maxIter=10,featuresCol='ScaledNumFeatures_ss_sqrt', labelCol='label')\n",
    "# 定义逻辑回归的pipeline\n",
    "pipeline_lr = Pipeline(stages=[indexer_gender, indexer_level, assembler_sqrt, scaler_ss_sqrt, lr])\n",
    "s = time()\n",
    "Model_lr_sqrt = pipeline_lr.fit(train)\n",
    "# 使用逻辑回归pipeline拟合训练数据\n",
    "e = time()\n",
    "print('time used:',e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_result_sqrt = Model_lr_sqrt.transform(validation)\n",
    "print('f1 score:',evaluator.evaluate(lr_result_sqrt, {evaluator.metricName: \"f1\"}))\n",
    "print('accuracy:',evaluator.evaluate(lr_result_sqrt, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化GBT\n",
    "gbt = GBTClassifier(maxIter=10,featuresCol='ScaledNumFeatures_ss',labelCol='label', seed=42)\n",
    "# 定义GBT的pipeline\n",
    "pipeline_gbt = Pipeline(stages=[indexer_gender, indexer_level, assembler, scaler_ss, gbt])\n",
    "s = time()\n",
    "# 使用GBT的ipeline拟合训练数据\n",
    "Model_gbt = pipeline_gbt.fit(train)\n",
    "e = time()\n",
    "print('time used:',e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbt_result = Model_gbt.transform(validation)\n",
    "print('f1 score:',evaluator.evaluate(gbt_result, {evaluator.metricName: \"f1\"}))\n",
    "print('accuracy:',evaluator.evaluate(gbt_result, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化支持向量机\n",
    "svm = LinearSVC(maxIter=10,featuresCol='ScaledNumFeatures_ss',labelCol='label')\n",
    "# 定义SVM的pipeline\n",
    "pipeline_svm = Pipeline(stages=[indexer_gender, indexer_level, assembler, scaler_ss, svm])\n",
    "s = time()\n",
    "# 使用SVM的pipeline拟合训练数据\n",
    "Model_svm = pipeline_svm.fit(train)\n",
    "e = time()\n",
    "print('time used:',e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svm_result = Model_svm.transform(validation)\n",
    "print('f1 score:',evaluator.evaluate(svm_result, {evaluator.metricName: \"f1\"}))\n",
    "print('accuracy:',evaluator.evaluate(svm_result, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化随机森林\n",
    "rf = RandomForestClassifier(featuresCol='ScaledNumFeatures_ss',labelCol='label', seed=42)\n",
    "# 定义随机森林的pipeline\n",
    "pipeline_rf = Pipeline(stages=[indexer_gender, indexer_level, assembler, scaler_ss, rf])\n",
    "s = time()\n",
    "# 使用随机森林pipeline拟合训练数据\n",
    "Model_rf = pipeline_rf.fit(train)\n",
    "e = time()\n",
    "print('time used:',e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_result = Model_rf.transform(validation)\n",
    "print('f1:',evaluator.evaluate(rf_result, {evaluator.metricName: \"f1\"}))\n",
    "print('accuracy:',evaluator.evaluate(rf_result, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认参数的模型在验证集上的表现：\n",
    "\n",
    "逻辑回归：\n",
    "- f1 score: 0.7678967239069541\n",
    "- accuracy: 0.8088235294117647\n",
    "\n",
    "GBT：\n",
    "- f1 score: 0.7116493656286045\n",
    "- accuracy: 0.7058823529411765\n",
    "\n",
    "支持向量机：\n",
    "- f1 score: 0.6627450980392157\n",
    "- accuracy: 0.7647058823529411\n",
    "\n",
    "随机森林：\n",
    "\n",
    "- f1: 0.8141923436041085\n",
    "- accuracy: 0.8235294117647058\n",
    "\n",
    "**可以看到随机森林相比其他三个有更高的F1得分，所以下一步用随机森林进行参数调整。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调整模型参数\n",
    "在随机森林上算法上使用网格搜索进行超参数选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol='ScaledNumFeatures_ss',labelCol='label', seed=42)\n",
    "paramGrid_tune = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [4,6,8]) \\\n",
    "    .addGrid(rf.maxDepth, [4,6,8]) \\\n",
    "    .build()\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "pipeline_rf = Pipeline(stages=[indexer_gender, indexer_level, assembler, scaler_ss, rf])\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid_tune,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "s = time()\n",
    "cvModel_rf_tune = crossval.fit(train)\n",
    "e = time()\n",
    "print('time used:',e-s)\n",
    "print('cv average f1_score',cvModel_rf_tune.avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_rf_result_tune = cvModel_rf_tune.bestModel.transform(test)\n",
    "print('f1:',evaluator.evaluate(best_rf_result_tune, {evaluator.metricName: \"f1\"}))\n",
    "print('accuracy:',evaluator.evaluate(best_rf_result_tune, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_rf_result_tune_df = best_rf_result_tune.toPandas()\n",
    "confusion_matrix(best_rf_result_tune_df['label'], best_rf_result_tune_df['prediction'], labels=[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整后的模型在测试机上的F1 score是0.784，accuracy是0.8，在测试集的15个样本中，有2个真实是1被预测为0，有1个真实是0被预测为1。最终结果的F1 socre比基准模型(F1 score:0.71)有提高，但是提高并不多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = cvModel_rf_tune.bestModel.stages[-1]\n",
    "importance = best_model.featureImportances.values\n",
    "feature_index = best_model.featureImportances.indices\n",
    "features = pd.Series(['avg_song_count','unique_session_num','add_playlist_num','add_friend_num','maxItemInSession',\n",
    "            'null_ratio','hours_since_reg','gender_index','level_index'])\n",
    "plt.barh(list(features[feature_index]) ,importance);\n",
    "plt.title('RandomForest feature importance ')\n",
    "plt.xlabel('importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林的feature_importance显示hour_since_reg对于用户是否会churn有着很大的影响，这是合理的，因为注册的时间长的用户代表用户在很长时间内都没有注销其账号，可能是经常使用，这样的用户更不容易注销账号，但也可能是很久以前注册过一直没用，也没有注销。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个notebook在128M的数据上训练了用于预测用户是否会churn的模型，评估了四个模型：逻辑回归，GBT，支持向量机，随机森林，通过对比F1 socre选择了随机森林并在其上调整参数，使用最优模型在测试集上得到的F1 score 是0.784，准确率是0.8，但是测试集只有15个样本，所以在测试集上的到的得分有较大偏差。最后列出随机森林给出的对于用户是否会churn的最重要特征是用户注册了多长时间(hour_since_reg)。此notebook使用的是数据的子集，这样才能在单台机器上运行，如果用完整数据(12G)，对于现有机器性能已经是大数据，需要在集群环境运行。数据有只225个用户样本，所以模型实际意义并不大，如果有更多数据，可以将本notebook拓展到spark环境，将对模型表现有很大程度的提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
